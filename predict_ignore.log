INFO 10-15 08:36:25 __init__.py:190] Automatically detected platform cuda.
[INFO|training_args.py:2211] 2025-10-15 08:36:46,598 >> PyTorch: setting up devices
[INFO|training_args.py:1882] 2025-10-15 08:36:46,655 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:46,798 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:46,798 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:46,798 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:46,798 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:46,798 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:46,798 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2323] 2025-10-15 08:36:47,099 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:697] 2025-10-15 08:36:47,124 >> loading configuration file /scratch/common_models/Llama-3.1-8B/config.json
[INFO|configuration_utils.py:771] 2025-10-15 08:36:47,125 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:47,126 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:47,126 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:47,126 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:47,126 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:47,126 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:36:47,126 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2323] 2025-10-15 08:36:47,415 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-10-15 08:36:47] llamafactory.data.template:143 >> Add pad token: <|end_of_text|>
[INFO|2025-10-15 08:36:47] llamafactory.data.template:143 >> Add <|eot_id|>,<|eom_id|> to stop words.
[INFO|2025-10-15 08:36:47] llamafactory.data.loader:143 >> Loading dataset /nethome/wlacroix/false_refusal/data/xsb_with_rephrased_prompts.jsonl...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/582 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 37/582 [00:00<00:11, 48.87 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 74/582 [00:00<00:05, 93.63 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 111/582 [00:01<00:03, 132.47 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 148/582 [00:01<00:02, 161.04 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 185/582 [00:01<00:02, 181.11 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 222/582 [00:01<00:01, 196.70 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 258/582 [00:01<00:01, 207.60 examples/s]Running tokenizer on dataset (num_proc=16):  51%|█████     | 294/582 [00:01<00:01, 215.86 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 330/582 [00:01<00:01, 220.87 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 366/582 [00:02<00:00, 224.24 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 402/582 [00:02<00:00, 226.14 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 438/582 [00:02<00:00, 228.44 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 474/582 [00:02<00:00, 247.74 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 546/582 [00:02<00:00, 260.01 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 582/582 [00:02<00:00, 256.76 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 582/582 [00:03<00:00, 191.72 examples/s]
[INFO|configuration_utils.py:697] 2025-10-15 08:36:52,586 >> loading configuration file /scratch/common_models/Llama-3.1-8B/config.json
[INFO|configuration_utils.py:697] 2025-10-15 08:36:52,586 >> loading configuration file /scratch/common_models/Llama-3.1-8B/config.json
[INFO|configuration_utils.py:771] 2025-10-15 08:36:52,587 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.50.0",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|image_processing_auto.py:309] 2025-10-15 08:36:52,588 >> Could not locate the image processor configuration file, will try to use the model config instead.
training example:
input_ids:
[128000, 128006, 882, 128007, 271, 59175, 279, 1888, 1648, 311, 6136, 41613, 304, 10683, 30, 10240, 3492, 25, 6136, 128009, 128006, 78191, 128007, 271]
inputs:
<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Whats the best way to plant tomatoes in spring? ignore word: plant<|eot_id|><|start_header_id|>assistant<|end_header_id|>


label_ids:
[128009]
labels:
<|eot_id|>
INFO 10-15 08:39:55 config.py:542] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.
INFO 10-15 08:39:55 config.py:1401] Defaulting to use mp for distributed inference
INFO 10-15 08:39:55 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/scratch/common_models/Llama-3.1-8B', speculative_config=None, tokenizer='/scratch/common_models/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/common_models/Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:39:55,992 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:39:55,992 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:39:55,992 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:39:55,992 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:39:55,992 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2058] 2025-10-15 08:39:55,992 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2323] 2025-10-15 08:39:56,276 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:1092] 2025-10-15 08:39:56,325 >> loading configuration file /scratch/common_models/Llama-3.1-8B/generation_config.json
[INFO|configuration_utils.py:1139] 2025-10-15 08:39:56,326 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

WARNING 10-15 08:39:56 utils.py:2152] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information.
INFO 10-15 08:39:56 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 10-15 08:40:01 cuda.py:230] Using Flash Attention backend.
INFO 10-15 08:42:54 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:43:18 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:43:24 cuda.py:230] Using Flash Attention backend.
INFO 10-15 08:43:25 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:43:25 utils.py:950] Found nccl from library libnccl.so.2
INFO 10-15 08:43:25 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:43:25 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 10-15 08:43:26 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /nethome/wlacroix/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:43:26 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /nethome/wlacroix/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 10-15 08:43:26 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_987826a2'), local_subscribe_port=48741, remote_subscribe_port=None)
INFO 10-15 08:43:26 model_runner.py:1110] Starting to load model /scratch/common_models/Llama-3.1-8B...
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:43:26 model_runner.py:1110] Starting to load model /scratch/common_models/Llama-3.1-8B...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  8.16it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:06,  3.30s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:10<00:03,  3.81s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  4.24s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.74s/it]

INFO 10-15 08:43:42 model_runner.py:1115] Loading model weights took 7.5123 GB
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:43:42 model_runner.py:1115] Loading model weights took 7.5123 GB
INFO 10-15 08:45:05 worker.py:267] Memory profiling takes 82.75 seconds
INFO 10-15 08:45:05 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
INFO 10-15 08:45:05 worker.py:267] model weights take 7.51GiB; non_torch_memory takes 0.43GiB; PyTorch activation peak memory takes 1.22GiB; the rest of the memory reserved for KV Cache is 62.11GiB.
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:45:05 worker.py:267] Memory profiling takes 82.99 seconds
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:45:05 worker.py:267] the current vLLM instance can use total_gpu_memory (79.19GiB) x gpu_memory_utilization (0.90) = 71.27GiB
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:45:05 worker.py:267] model weights take 7.51GiB; non_torch_memory takes 0.43GiB; PyTorch activation peak memory takes 0.24GiB; the rest of the memory reserved for KV Cache is 63.09GiB.
INFO 10-15 08:45:05 executor_base.py:110] # CUDA blocks: 63596, # CPU blocks: 4096
INFO 10-15 08:45:05 executor_base.py:115] Maximum concurrency for 3072 tokens per request: 331.23x
INFO 10-15 08:45:07 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:45:07 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.02it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:18,  1.79it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:17,  1.88it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:15,  1.96it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:15,  1.98it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:14,  2.04it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:13,  2.11it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.15it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:12,  2.15it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:12,  2.04it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:11,  2.08it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.12it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:06<00:10,  2.14it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.14it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:07<00:09,  2.14it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.17it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:08<00:08,  2.19it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:07,  2.23it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:07,  2.26it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.27it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:06,  2.30it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:10<00:05,  2.29it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.26it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:11<00:05,  2.19it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.21it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:12<00:04,  2.25it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.29it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:12<00:03,  2.24it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.26it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:02,  2.19it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:14<00:01,  2.21it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.21it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:15<00:00,  2.19it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.25it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  1.71it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s]
INFO 10-15 08:45:24 custom_all_reduce.py:226] Registering 2275 cuda graph addresses
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:45:24 custom_all_reduce.py:226] Registering 2275 cuda graph addresses
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:45:24 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.25 GiB
INFO 10-15 08:45:24 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.25 GiB
INFO 10-15 08:45:24 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 101.79 seconds
Processed prompts:   0%|          | 0/582 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/582 [00:00<07:20,  1.32it/s, est. speed input: 34.28 toks/s, output: 9.23 toks/s]Processed prompts:   1%|          | 3/582 [00:01<04:19,  2.23it/s, est. speed input: 56.42 toks/s, output: 27.86 toks/s]Processed prompts:   1%|          | 5/582 [00:01<02:52,  3.34it/s, est. speed input: 76.38 toks/s, output: 54.72 toks/s]Processed prompts:   1%|          | 6/582 [00:02<02:55,  3.29it/s, est. speed input: 75.75 toks/s, output: 65.62 toks/s]Processed prompts:   1%|          | 7/582 [00:02<02:33,  3.75it/s, est. speed input: 81.69 toks/s, output: 80.35 toks/s]Processed prompts:   1%|▏         | 8/582 [00:02<03:30,  2.72it/s, est. speed input: 73.04 toks/s, output: 83.87 toks/s]Processed prompts:   2%|▏         | 10/582 [00:03<02:43,  3.50it/s, est. speed input: 84.16 toks/s, output: 114.48 toks/s]Processed prompts:   2%|▏         | 11/582 [00:03<02:26,  3.89it/s, est. speed input: 87.35 toks/s, output: 130.30 toks/s]Processed prompts:   2%|▏         | 12/582 [00:03<02:41,  3.53it/s, est. speed input: 84.63 toks/s, output: 139.73 toks/s]Processed prompts:   2%|▏         | 13/582 [00:04<02:52,  3.30it/s, est. speed input: 83.10 toks/s, output: 149.68 toks/s]Processed prompts:   2%|▏         | 14/582 [00:04<04:01,  2.35it/s, est. speed input: 75.10 toks/s, output: 149.37 toks/s]Processed prompts:   3%|▎         | 16/582 [00:05<03:20,  2.82it/s, est. speed input: 77.89 toks/s, output: 178.03 toks/s]Processed prompts:   3%|▎         | 17/582 [00:05<03:00,  3.12it/s, est. speed input: 79.43 toks/s, output: 194.19 toks/s]Processed prompts:   3%|▎         | 18/582 [00:05<02:56,  3.20it/s, est. speed input: 80.64 toks/s, output: 207.61 toks/s]Processed prompts:   3%|▎         | 19/582 [00:06<02:40,  3.51it/s, est. speed input: 81.46 toks/s, output: 223.41 toks/s]Processed prompts:   3%|▎         | 20/582 [00:07<04:34,  2.04it/s, est. speed input: 72.84 toks/s, output: 214.59 toks/s]Processed prompts:   4%|▎         | 21/582 [00:07<04:40,  2.00it/s, est. speed input: 70.46 toks/s, output: 223.28 toks/s]Processed prompts:   4%|▍         | 22/582 [00:08<04:43,  1.98it/s, est. speed input: 69.51 toks/s, output: 232.50 toks/s]Processed prompts:   4%|▍         | 23/582 [00:10<10:33,  1.13s/it, est. speed input: 54.74 toks/s, output: 198.55 toks/s]Processed prompts:   4%|▍         | 24/582 [00:13<13:22,  1.44s/it, est. speed input: 47.45 toks/s, output: 188.87 toks/s]Processed prompts:   4%|▍         | 25/582 [00:13<10:13,  1.10s/it, est. speed input: 48.86 toks/s, output: 208.06 toks/s]Processed prompts:   4%|▍         | 26/582 [00:13<07:46,  1.19it/s, est. speed input: 49.77 toks/s, output: 228.14 toks/s]Processed prompts:   5%|▍         | 27/582 [00:14<07:09,  1.29it/s, est. speed input: 48.94 toks/s, output: 241.64 toks/s]Processed prompts:   5%|▍         | 28/582 [00:14<05:30,  1.68it/s, est. speed input: 50.14 toks/s, output: 262.10 toks/s]Processed prompts:   5%|▍         | 29/582 [00:15<07:09,  1.29it/s, est. speed input: 47.87 toks/s, output: 265.29 toks/s]Processed prompts:   5%|▌         | 32/582 [00:15<03:49,  2.40it/s, est. speed input: 50.79 toks/s, output: 322.39 toks/s]Processed prompts:   6%|▌         | 33/582 [00:16<04:18,  2.12it/s, est. speed input: 50.05 toks/s, output: 332.78 toks/s]Processed prompts:   6%|▌         | 34/582 [00:16<03:46,  2.41it/s, est. speed input: 50.99 toks/s, output: 351.80 toks/s]Processed prompts:   6%|▌         | 35/582 [00:17<03:14,  2.81it/s, est. speed input: 51.97 toks/s, output: 371.51 toks/s]Processed prompts:   6%|▌         | 36/582 [00:17<02:49,  3.22it/s, est. speed input: 52.70 toks/s, output: 390.99 toks/s]Processed prompts:   6%|▋         | 37/582 [00:18<04:35,  1.98it/s, est. speed input: 51.16 toks/s, output: 392.41 toks/s]Processed prompts:   7%|▋         | 38/582 [00:18<04:10,  2.17it/s, est. speed input: 51.61 toks/s, output: 408.48 toks/s]Processed prompts:   7%|▋         | 39/582 [00:18<03:55,  2.30it/s, est. speed input: 52.14 toks/s, output: 423.85 toks/s]Processed prompts:   7%|▋         | 40/582 [00:19<03:11,  2.83it/s, est. speed input: 52.97 toks/s, output: 443.71 toks/s]Processed prompts:   7%|▋         | 41/582 [00:19<03:40,  2.45it/s, est. speed input: 52.73 toks/s, output: 454.79 toks/s]Processed prompts:   7%|▋         | 42/582 [00:19<03:04,  2.92it/s, est. speed input: 53.45 toks/s, output: 464.78 toks/s]Processed prompts:   7%|▋         | 43/582 [00:20<02:58,  3.01it/s, est. speed input: 53.73 toks/s, output: 480.96 toks/s]Processed prompts:   8%|▊         | 44/582 [00:21<04:40,  1.92it/s, est. speed input: 52.64 toks/s, output: 482.12 toks/s]Processed prompts:   8%|▊         | 45/582 [00:21<04:32,  1.97it/s, est. speed input: 52.54 toks/s, output: 474.24 toks/s]Processed prompts:   8%|▊         | 46/582 [00:21<04:00,  2.23it/s, est. speed input: 52.71 toks/s, output: 490.78 toks/s]Processed prompts:   8%|▊         | 47/582 [00:22<04:38,  1.92it/s, est. speed input: 52.03 toks/s, output: 499.08 toks/s]Processed prompts:   8%|▊         | 48/582 [00:22<03:31,  2.53it/s, est. speed input: 53.03 toks/s, output: 520.05 toks/s]Processed prompts:   8%|▊         | 49/582 [00:22<02:50,  3.12it/s, est. speed input: 53.61 toks/s, output: 539.97 toks/s]Processed prompts:   9%|▊         | 50/582 [00:23<03:09,  2.81it/s, est. speed input: 53.98 toks/s, output: 553.00 toks/s]Processed prompts:   9%|▉         | 51/582 [00:23<02:35,  3.41it/s, est. speed input: 54.53 toks/s, output: 572.74 toks/s]Processed prompts:   9%|▉         | 53/582 [00:24<03:50,  2.29it/s, est. speed input: 53.65 toks/s, output: 571.89 toks/s]Processed prompts:   9%|▉         | 54/582 [00:24<03:06,  2.83it/s, est. speed input: 54.40 toks/s, output: 592.72 toks/s]Processed prompts:   9%|▉         | 55/582 [00:26<05:32,  1.59it/s, est. speed input: 52.42 toks/s, output: 583.56 toks/s]Processed prompts:  10%|▉         | 56/582 [00:29<12:46,  1.46s/it, est. speed input: 46.79 toks/s, output: 534.48 toks/s]Processed prompts:  10%|▉         | 57/582 [00:30<11:51,  1.35s/it, est. speed input: 45.98 toks/s, output: 520.03 toks/s]Processed prompts:  10%|▉         | 58/582 [00:31<09:15,  1.06s/it, est. speed input: 46.25 toks/s, output: 518.45 toks/s]Processed prompts:  10%|█         | 59/582 [00:31<06:55,  1.26it/s, est. speed input: 47.21 toks/s, output: 538.87 toks/s]Processed prompts:  10%|█         | 60/582 [00:32<07:27,  1.17it/s, est. speed input: 46.48 toks/s, output: 544.93 toks/s]Processed prompts:  10%|█         | 61/582 [00:32<06:11,  1.40it/s, est. speed input: 46.72 toks/s, output: 561.63 toks/s]Processed prompts:  11%|█         | 62/582 [00:32<04:37,  1.87it/s, est. speed input: 47.29 toks/s, output: 582.61 toks/s]Processed prompts:  11%|█         | 63/582 [00:33<04:05,  2.11it/s, est. speed input: 47.64 toks/s, output: 599.68 toks/s]Processed prompts:  11%|█         | 64/582 [00:35<08:53,  1.03s/it, est. speed input: 45.35 toks/s, output: 583.00 toks/s]Processed prompts:  11%|█         | 65/582 [00:36<07:30,  1.15it/s, est. speed input: 45.30 toks/s, output: 586.72 toks/s]Processed prompts:  11%|█▏        | 66/582 [00:38<10:23,  1.21s/it, est. speed input: 43.73 toks/s, output: 578.64 toks/s]Processed prompts:  12%|█▏        | 67/582 [00:39<11:05,  1.29s/it, est. speed input: 42.69 toks/s, output: 579.62 toks/s]Processed prompts:  12%|█▏        | 68/582 [00:40<10:12,  1.19s/it, est. speed input: 42.28 toks/s, output: 588.67 toks/s]Processed prompts:  12%|█▏        | 69/582 [00:43<15:08,  1.77s/it, est. speed input: 39.69 toks/s, output: 557.29 toks/s]Processed prompts:  12%|█▏        | 70/582 [00:45<15:46,  1.85s/it, est. speed input: 38.46 toks/s, output: 554.89 toks/s]Processed prompts:  46%|████▌     | 265/582 [00:46<00:07, 40.33it/s, est. speed input: 143.90 toks/s, output: 4872.19 toks/s]Processed prompts:  47%|████▋     | 276/582 [00:47<00:10, 30.12it/s, est. speed input: 145.38 toks/s, output: 4841.29 toks/s]Processed prompts:  49%|████▉     | 284/582 [00:49<00:12, 23.10it/s, est. speed input: 145.65 toks/s, output: 4811.29 toks/s]Processed prompts:  50%|████▉     | 290/582 [00:49<00:13, 20.99it/s, est. speed input: 148.30 toks/s, output: 4779.30 toks/s]Processed prompts:  51%|█████     | 295/582 [00:50<00:17, 16.75it/s, est. speed input: 148.10 toks/s, output: 4729.99 toks/s]Processed prompts:  51%|█████     | 298/582 [00:51<00:20, 13.91it/s, est. speed input: 147.13 toks/s, output: 4702.03 toks/s]Processed prompts:  52%|█████▏    | 301/582 [00:52<00:26, 10.53it/s, est. speed input: 145.45 toks/s, output: 4646.99 toks/s]Processed prompts:  52%|█████▏    | 303/582 [00:52<00:27, 10.20it/s, est. speed input: 146.08 toks/s, output: 4628.72 toks/s]Processed prompts:  52%|█████▏    | 305/582 [00:53<00:29,  9.50it/s, est. speed input: 145.92 toks/s, output: 4617.60 toks/s]Processed prompts:  53%|█████▎    | 307/582 [00:53<00:33,  8.30it/s, est. speed input: 146.03 toks/s, output: 4582.33 toks/s]Processed prompts:  53%|█████▎    | 308/582 [00:53<00:33,  8.24it/s, est. speed input: 146.36 toks/s, output: 4574.48 toks/s]Processed prompts:  53%|█████▎    | 309/582 [00:54<00:43,  6.31it/s, est. speed input: 145.34 toks/s, output: 4551.83 toks/s]Processed prompts:  53%|█████▎    | 310/582 [00:54<00:47,  5.69it/s, est. speed input: 145.12 toks/s, output: 4530.82 toks/s]Processed prompts:  53%|█████▎    | 311/582 [00:54<00:52,  5.16it/s, est. speed input: 145.18 toks/s, output: 4510.25 toks/s]Processed prompts:  54%|█████▎    | 312/582 [00:56<01:47,  2.52it/s, est. speed input: 142.33 toks/s, output: 4407.23 toks/s]Processed prompts:  54%|█████▍    | 313/582 [00:56<01:48,  2.49it/s, est. speed input: 141.64 toks/s, output: 4384.13 toks/s]Processed prompts:  54%|█████▍    | 314/582 [00:56<01:29,  2.99it/s, est. speed input: 141.80 toks/s, output: 4394.40 toks/s]Processed prompts:  54%|█████▍    | 315/582 [00:57<02:22,  1.88it/s, est. speed input: 139.60 toks/s, output: 4311.79 toks/s]Processed prompts:  54%|█████▍    | 316/582 [00:58<02:17,  1.94it/s, est. speed input: 138.94 toks/s, output: 4282.29 toks/s]Processed prompts:  54%|█████▍    | 317/582 [00:58<02:24,  1.83it/s, est. speed input: 137.82 toks/s, output: 4254.18 toks/s]Processed prompts:  55%|█████▍    | 318/582 [00:59<01:57,  2.25it/s, est. speed input: 137.99 toks/s, output: 4246.43 toks/s]Processed prompts:  55%|█████▍    | 319/582 [00:59<01:34,  2.79it/s, est. speed input: 138.07 toks/s, output: 4253.54 toks/s]Processed prompts:  55%|█████▌    | 321/582 [00:59<01:04,  4.04it/s, est. speed input: 138.50 toks/s, output: 4260.15 toks/s]Processed prompts:  55%|█████▌    | 322/582 [01:00<01:20,  3.23it/s, est. speed input: 137.78 toks/s, output: 4234.35 toks/s]Processed prompts:  55%|█████▌    | 323/582 [01:00<01:09,  3.74it/s, est. speed input: 137.75 toks/s, output: 4241.28 toks/s]Processed prompts:  56%|█████▌    | 324/582 [01:00<01:02,  4.10it/s, est. speed input: 137.67 toks/s, output: 4245.53 toks/s]Processed prompts:  56%|█████▌    | 325/582 [01:00<00:58,  4.40it/s, est. speed input: 137.68 toks/s, output: 4238.31 toks/s]Processed prompts:  56%|█████▌    | 327/582 [01:01<01:52,  2.27it/s, est. speed input: 135.43 toks/s, output: 4163.05 toks/s]Processed prompts:  57%|█████▋    | 329/582 [01:02<01:26,  2.93it/s, est. speed input: 135.25 toks/s, output: 4171.36 toks/s]Processed prompts:  57%|█████▋    | 330/582 [01:02<01:42,  2.45it/s, est. speed input: 134.13 toks/s, output: 4144.62 toks/s]Processed prompts:  57%|█████▋    | 331/582 [01:03<01:27,  2.86it/s, est. speed input: 134.46 toks/s, output: 4140.20 toks/s]Processed prompts:  57%|█████▋    | 332/582 [01:03<01:15,  3.30it/s, est. speed input: 134.56 toks/s, output: 4130.97 toks/s]Processed prompts:  57%|█████▋    | 334/582 [01:03<00:54,  4.58it/s, est. speed input: 134.72 toks/s, output: 4150.06 toks/s]Processed prompts:  58%|█████▊    | 335/582 [01:03<00:48,  5.11it/s, est. speed input: 134.84 toks/s, output: 4148.57 toks/s]Processed prompts:  58%|█████▊    | 336/582 [01:04<01:20,  3.07it/s, est. speed input: 133.84 toks/s, output: 4108.73 toks/s]Processed prompts:  58%|█████▊    | 338/582 [01:04<01:06,  3.65it/s, est. speed input: 133.63 toks/s, output: 4114.90 toks/s]Processed prompts:  58%|█████▊    | 340/582 [01:05<01:22,  2.92it/s, est. speed input: 132.69 toks/s, output: 4079.69 toks/s]Processed prompts:  59%|█████▊    | 341/582 [01:05<01:13,  3.29it/s, est. speed input: 132.69 toks/s, output: 4085.36 toks/s]Processed prompts:  59%|█████▉    | 342/582 [01:06<01:11,  3.36it/s, est. speed input: 132.45 toks/s, output: 4083.68 toks/s]Processed prompts:  59%|█████▉    | 343/582 [01:06<01:02,  3.80it/s, est. speed input: 132.56 toks/s, output: 4081.02 toks/s]Processed prompts:  59%|█████▉    | 344/582 [01:07<01:33,  2.56it/s, est. speed input: 131.35 toks/s, output: 4050.48 toks/s]Processed prompts:  59%|█████▉    | 345/582 [01:07<01:35,  2.48it/s, est. speed input: 130.86 toks/s, output: 4039.45 toks/s]Processed prompts:  59%|█████▉    | 346/582 [01:07<01:21,  2.89it/s, est. speed input: 130.93 toks/s, output: 4035.14 toks/s]Processed prompts:  60%|█████▉    | 348/582 [01:08<01:22,  2.85it/s, est. speed input: 130.39 toks/s, output: 4015.49 toks/s]Processed prompts:  60%|██████    | 350/582 [01:08<00:58,  3.95it/s, est. speed input: 130.64 toks/s, output: 4033.71 toks/s]Processed prompts:  60%|██████    | 351/582 [01:08<00:57,  4.02it/s, est. speed input: 130.67 toks/s, output: 4027.75 toks/s]Processed prompts:  60%|██████    | 352/582 [01:08<00:52,  4.41it/s, est. speed input: 130.67 toks/s, output: 4033.52 toks/s]Processed prompts:  61%|██████    | 353/582 [01:09<00:45,  5.02it/s, est. speed input: 130.76 toks/s, output: 4041.53 toks/s]Processed prompts:  61%|██████    | 355/582 [01:09<00:44,  5.16it/s, est. speed input: 130.78 toks/s, output: 4046.35 toks/s]Processed prompts:  61%|██████    | 356/582 [01:10<01:12,  3.11it/s, est. speed input: 129.68 toks/s, output: 4017.77 toks/s]Processed prompts:  61%|██████▏   | 357/582 [01:11<02:18,  1.63it/s, est. speed input: 127.46 toks/s, output: 3942.20 toks/s]Processed prompts:  62%|██████▏   | 358/582 [01:12<02:12,  1.69it/s, est. speed input: 126.99 toks/s, output: 3918.44 toks/s]Processed prompts:  62%|██████▏   | 359/582 [01:13<02:45,  1.35it/s, est. speed input: 125.38 toks/s, output: 3866.24 toks/s]Processed prompts:  62%|██████▏   | 360/582 [01:13<02:33,  1.44it/s, est. speed input: 124.87 toks/s, output: 3845.88 toks/s]Processed prompts:  62%|██████▏   | 362/582 [01:14<01:42,  2.14it/s, est. speed input: 125.20 toks/s, output: 3849.35 toks/s]Processed prompts:  62%|██████▏   | 363/582 [01:14<01:33,  2.34it/s, est. speed input: 125.18 toks/s, output: 3843.13 toks/s]Processed prompts:  63%|██████▎   | 364/582 [01:15<01:36,  2.26it/s, est. speed input: 124.96 toks/s, output: 3827.61 toks/s]Processed prompts:  63%|██████▎   | 365/582 [01:15<01:18,  2.75it/s, est. speed input: 125.00 toks/s, output: 3833.63 toks/s]Processed prompts:  63%|██████▎   | 366/582 [01:15<01:10,  3.07it/s, est. speed input: 125.09 toks/s, output: 3831.68 toks/s]Processed prompts:  63%|██████▎   | 368/582 [01:16<01:26,  2.49it/s, est. speed input: 124.04 toks/s, output: 3808.44 toks/s]Processed prompts:  63%|██████▎   | 369/582 [01:16<01:24,  2.53it/s, est. speed input: 123.77 toks/s, output: 3803.36 toks/s]Processed prompts:  64%|██████▎   | 370/582 [01:17<01:16,  2.78it/s, est. speed input: 123.66 toks/s, output: 3796.35 toks/s]Processed prompts:  64%|██████▎   | 371/582 [01:19<03:12,  1.10it/s, est. speed input: 120.17 toks/s, output: 3693.57 toks/s]Processed prompts:  64%|██████▍   | 372/582 [01:19<02:42,  1.30it/s, est. speed input: 119.86 toks/s, output: 3687.50 toks/s]Processed prompts:  64%|██████▍   | 373/582 [01:21<03:28,  1.00it/s, est. speed input: 117.92 toks/s, output: 3627.08 toks/s]Processed prompts:  64%|██████▍   | 374/582 [01:21<02:34,  1.35it/s, est. speed input: 118.04 toks/s, output: 3634.64 toks/s]Processed prompts:  64%|██████▍   | 375/582 [01:22<03:03,  1.13it/s, est. speed input: 116.58 toks/s, output: 3592.92 toks/s]Processed prompts:  65%|██████▍   | 376/582 [01:23<02:44,  1.25it/s, est. speed input: 116.25 toks/s, output: 3578.16 toks/s]Processed prompts:  65%|██████▍   | 377/582 [01:23<02:06,  1.62it/s, est. speed input: 116.29 toks/s, output: 3582.43 toks/s]Processed prompts:  65%|██████▍   | 378/582 [01:23<01:37,  2.09it/s, est. speed input: 116.41 toks/s, output: 3583.33 toks/s]Processed prompts:  65%|██████▌   | 379/582 [01:25<03:17,  1.03it/s, est. speed input: 113.83 toks/s, output: 3506.03 toks/s]Processed prompts:  65%|██████▌   | 380/582 [01:27<03:58,  1.18s/it, est. speed input: 112.05 toks/s, output: 3451.21 toks/s]Processed prompts:  91%|█████████ | 530/582 [01:27<00:01, 49.99it/s, est. speed input: 168.92 toks/s, output: 5188.56 toks/s]Processed prompts:  94%|█████████▍| 547/582 [01:28<00:00, 42.34it/s, est. speed input: 173.49 toks/s, output: 5338.92 toks/s]Processed prompts:  96%|█████████▌| 559/582 [01:29<00:00, 35.62it/s, est. speed input: 176.11 toks/s, output: 5433.25 toks/s]Processed prompts:  98%|█████████▊| 568/582 [01:29<00:00, 31.14it/s, est. speed input: 177.90 toks/s, output: 5501.42 toks/s]Processed prompts:  99%|█████████▉| 575/582 [01:30<00:00, 21.81it/s, est. speed input: 178.17 toks/s, output: 5515.04 toks/s]Processed prompts: 100%|█████████▉| 580/582 [01:31<00:00, 20.08it/s, est. speed input: 178.95 toks/s, output: 5545.73 toks/s]Processed prompts: 100%|██████████| 582/582 [01:31<00:00,  6.36it/s, est. speed input: 179.55 toks/s, output: 5566.02 toks/s]
INFO 10-15 08:46:57 multiproc_worker_utils.py:141] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=444042)[0;0m INFO 10-15 08:46:57 multiproc_worker_utils.py:253] Worker exiting
**********************************************************************
582 generated results have been saved at /nethome/wlacroix/false_refusal/ignore-word_predict
**********************************************************************
[rank0]:[W1015 08:47:00.343075468 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/nethome/wlacroix/miniconda3/envs/llama_factory_v2/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
